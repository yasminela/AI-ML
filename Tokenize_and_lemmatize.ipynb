{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4ZdL9Fn065gilDq6duunK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasminela/AI-ML/blob/main/Tokenize_and_lemmatize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.exceptions import NotFittedError\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "import joblib\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define constants\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "LEMMATIZER = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def load_data(train_path, test_path):\n",
        "    \"\"\"Load train and test datasets.\"\"\"\n",
        "    try:\n",
        "        train_df = pd.read_csv('/content/train.csv')\n",
        "        test_df = pd.read_csv('/content/train.csv')\n",
        "        logging.info(\"Datasets loaded successfully.\")\n",
        "        return train_df, test_df\n",
        "    except FileNotFoundError as e:\n",
        "        logging.error(f\"File not found: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text: lowercase, remove punctuation, lemmatize, and remove stopwords.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize and lemmatize\n",
        "    words = text.split()\n",
        "    words = [LEMMATIZER.lemmatize(word) for word in words if word not in STOPWORDS]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "def prepare_data(df):\n",
        "    \"\"\"Prepare data by combining title and content, filling missing values, and preprocessing text.\"\"\"\n",
        "    df['title'] = df['title'].fillna('').astype(str)\n",
        "    df['content'] = df['content'].fillna('').astype(str)\n",
        "    df['text'] = df['title'] + ' ' + df['content']\n",
        "    df['text_processed'] = df['text'].apply(preprocess_text)\n",
        "    return df\n",
        "\n",
        "\n",
        "def encode_labels(y, label_encoder=None):\n",
        "    \"\"\"Encode target labels.\"\"\"\n",
        "    if label_encoder is None:\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_encoded = label_encoder.fit_transform(y)\n",
        "    else:\n",
        "        try:\n",
        "            y_encoded = label_encoder.transform(y)\n",
        "        except ValueError:\n",
        "            logging.error(\"Label encoder not fitted or unknown labels encountered.\")\n",
        "            raise\n",
        "    return y_encoded, label_encoder\n",
        "\n",
        "\n",
        "def build_pipeline():\n",
        "    \"\"\"Build the machine learning pipeline.\"\"\"\n",
        "    pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
        "        ('clf', LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='auto'))\n",
        "    ])\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_val, y_val, label_encoder):\n",
        "    \"\"\"Evaluate the model on validation data.\"\"\"\n",
        "    try:\n",
        "        y_pred = model.predict(X_val)\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        report = classification_report(y_val, y_pred, target_names=label_encoder.classes_, output_dict=True)\n",
        "        logging.info(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "        logging.info(\"Classification Report:\\n\" + pd.DataFrame(report).T.to_string())\n",
        "        return accuracy, report\n",
        "    except NotFittedError:\n",
        "        logging.error(\"Model is not fitted yet.\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def tune_hyperparameters(pipeline, X_train, y_train):\n",
        "    \"\"\"Tune hyperparameters using GridSearchCV.\"\"\"\n",
        "    param_grid = {\n",
        "        'tfidf__max_features': [5000, 10000],\n",
        "        'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "        'clf__C': [0.1, 1, 10],\n",
        "        'clf__solver': ['lbfgs', 'saga']\n",
        "    }\n",
        "    scorer = make_scorer(accuracy_score)\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=StratifiedKFold(n_splits=5), scoring=scorer, n_jobs=-1)\n",
        "    grid_search.fit(X_train,y_train)\n",
        "    logging.info(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "\n",
        "def save_model(model, label_encoder, model_path=\"model.joblib\", encoder_path=\"label_encoder.joblib\"):\n",
        "    \"\"\"Save the trained model and label encoder.\"\"\"\n",
        "    joblib.dump(model, model_path)\n",
        "    joblib.dump(label_encoder, encoder_path)\n",
        "    logging.info(\"Model and label encoder saved successfully.\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load datasets\n",
        "    train_path = '/train.csv'\n",
        "    test_path = '/test.csv'\n",
        "    train_df, test_df = load_data(train_path, test_path)\n",
        "\n",
        "    # Prepare data\n",
        "    train_df = prepare_data(train_df)\n",
        "    test_df = prepare_data(test_df)\n",
        "\n",
        "    # Encode target labels\n",
        "    y_train, label_encoder = encode_labels(train_df['target'])\n",
        "    X_train = train_df['text_processed']\n",
        "\n",
        "    # Split data into train and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
        "    )\n",
        "\n",
        "    # Build and tune the pipeline\n",
        "    pipeline = build_pipeline()\n",
        "    tuned_pipeline = tune_hyperparameters(pipeline, X_train, y_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate_model(tuned_pipeline, X_val, y_val, label_encoder)\n",
        "\n",
        "    # Predict on test data\n",
        "    X_test = test_df['text_processed']\n",
        "    test_predictions = tuned_pipeline.predict(X_test)\n",
        "    predicted_categories = label_encoder.inverse_transform(test_predictions)\n",
        "\n",
        "    # Create submission file\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': test_df['id'],\n",
        "        'target': predicted_categories\n",
        "    })\n",
        "    submission_df = submission_df.sort_values(by='id').reset_index(drop=True)\n",
        "    submission_df.to_csv('submission.csv', index=False)\n",
        "    logging.info(\"Submission file saved successfully.\")\n",
        "\n",
        "    # Save the model and label encoder\n",
        "    save_model(tuned_pipeline, label_encoder)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2NyJh9oKXCv",
        "outputId": "380d6ddf-392b-4707-a879-ce7d5d99c424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}